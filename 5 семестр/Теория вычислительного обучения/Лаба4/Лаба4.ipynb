{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9d12ca",
   "metadata": {},
   "source": [
    "<center><h1>Обучение с подкреплением<h1/><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed895c",
   "metadata": {},
   "source": [
    "<img src=\"https://sun9-8.userapi.com/impg/zOa9OmJAuFXbDMEvcWVmwp45e2qvJ5MxojsyZw/Hta-Zpo2KSE.jpg?size=1080x1005&quality=96&sign=3320b562fcdb238d4c5a621b55fa8c9c&type=album\" height=720 width=670/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee0be1",
   "metadata": {},
   "source": [
    "## Инициализация игры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36816d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "import statistics\n",
    "import random\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83b01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d8928",
   "metadata": {},
   "source": [
    "Данная игра представляет из себя каретку, на которой расположена палка в состоянии неравновесия. Наша цель – не позволить палке упасть. Мы можем достичь этого, двигая каретку влево либо вправо. Посмотрим, как выглядит эта игра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff5ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alekc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        a = env.action_space.sample()\n",
    "        env.step(a)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a5995",
   "metadata": {},
   "source": [
    "Попробуем вручную подобрать тактику. Пусть каждый раз, когда палка отклоняется влево, каретка также начинается двигаться влево и наоборот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59670455",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        action = 0 if observation[2] < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2efe5",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df824655",
   "metadata": {},
   "source": [
    "Для того чтобы обучить нашего агента, будем использовать Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19580579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "\n",
    "    def __init__(self, discount, \n",
    "                 get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._q_values = \\\n",
    "            defaultdict(lambda: defaultdict(lambda: 0))  \n",
    "        self.discount = discount\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self._q_values[state][action]\n",
    "\n",
    "    def set_q_value(self, state, action, value):\n",
    "        self._q_values[state][action] = value\n",
    "        \n",
    "    def _calculate_alpha(self, n, min_rate=0.01 ) -> float  :\n",
    "        \"\"\"Адаптивно считает гиперпараметр альфа (коэффициент обучаемости)\"\"\"\n",
    "        \n",
    "        return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n",
    "    \n",
    "    def _calculate_epsilon(self, n, min_rate=0.1 ) -> float  :\n",
    "        \"\"\"Адаптивно считает гиперпараметр эпсилон (коэффициент случайности)\"\"\"\n",
    "        return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))\n",
    "        \n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "          Возвращает значение функции полезности, \n",
    "          рассчитанной по Q[state, action]\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        value = max([self.get_q_value(state, action) for action in possible_actions])\n",
    "        return value\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        \"\"\"\n",
    "          Выбирает лучшее действие, согласно стратегии.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        \n",
    "        if all( self.get_q_value(state, action) ==  self.get_q_value(state, possible_actions[0]) for action in possible_actions):\n",
    "            return random.choice(possible_actions)\n",
    "\n",
    "        best_action = None\n",
    "        for action in possible_actions:\n",
    "            if best_action is None:\n",
    "                best_action = action\n",
    "            elif self.get_q_value(state, action) > self.get_q_value(state, best_action):\n",
    "                best_action = action\n",
    "        return best_action\n",
    "        \n",
    "    \n",
    "    def get_action(self, n, state):\n",
    "        \"\"\"\n",
    "          Выбирает действие, предпринимаемое в данном \n",
    "          состоянии, включая исследование (eps greedy)\n",
    "          С вероятностью self.epsilon берем случайное \n",
    "          действие, иначе действие согласно стратегии \n",
    "          (self.get_policy)\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        if np.random.random() < self._calculate_epsilon(n):\n",
    "            action = np.random.choice(possible_actions, 1)[0]\n",
    "        else:\n",
    "            action = self.get_policy(state)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self, n, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "          Функция Q-обновления \n",
    "        \"\"\"\n",
    "        alpha = self._calculate_alpha(n)\n",
    "        \n",
    "        learnt_value = reward + self.discount * self.get_value(next_state)\n",
    "        old_value = self.get_q_value(state, action)\n",
    "        reference_qvalue = (1 - alpha) * old_value + alpha * learnt_value\n",
    "        \n",
    "        self.set_q_value(state, action, reference_qvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e778246",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672241d",
   "metadata": {},
   "source": [
    "Обучение с дообучением может быть использовано лишь на конечном множестве состояний. Однако положение палки – величина дискретная. Поэтому нам необходимо разбить наше дискретной множество признаков на конечное (и относительно небольшое) количество состояний-комбинаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d6426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6 , 12)\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "est.fit([lower_bounds, upper_bounds ])\n",
    "\n",
    "def discreditize(obs):\n",
    "    return tuple(map(int,est.transform([[obs[2], obs[3]]])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e2635",
   "metadata": {},
   "source": [
    "## Непосредственно обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3362912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_in_range(env: gym.wrappers.time_limit.TimeLimit, pos: int, range_: tuple):\n",
    "    if range_[0] <= pos < range_[1]:\n",
    "        env.render()\n",
    "    elif pos == range_[1]:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3992a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(discount=1,\n",
    "                       get_legal_actions=lambda s: range(\n",
    "                           env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357dfb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "215.0\n",
      "58.0\n",
      "500.0\n",
      "500.0\n",
      "456.0\n",
      "416.0\n",
      "500.0\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1000):       \n",
    "    total_reward, done = 0, False\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        angle = discreditize(observation)\n",
    "\n",
    "        a = agent.get_action(iteration, angle)\n",
    "        next_observation, reward, done, _ = env.step(a)\n",
    "\n",
    "        next_angle = discreditize(next_observation)\n",
    "        \n",
    "        agent.update(iteration, angle, a, next_angle, reward) \n",
    "        \n",
    "#         reward *= 1 - abs(observation[0]) / 4.8\n",
    "        \n",
    "        render_in_range(env, iteration, (100, 105))\n",
    "        \n",
    "        render_in_range(env, iteration, (200, 204))\n",
    "        \n",
    "        render_in_range(env, iteration, (500, 503))\n",
    "\n",
    "        observation = next_observation\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "            \n",
    "    if iteration % 50 == 0 and iteration != 0:\n",
    "        print(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea7582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de0bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
